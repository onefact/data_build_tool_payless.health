{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The sql extension is already loaded. To reload it, use:\n",
      "  %reload_ext sql\n"
     ]
    }
   ],
   "source": [
    "# smallest possible test case for SQL query using jupyter notebook cell magic commands\n",
    "import duckdb\n",
    "import pandas as pd\n",
    "import requests\n",
    "import chardet\n",
    "from tqdm import tqdm\n",
    "from dotenv import load_dotenv\n",
    "import boto3\n",
    "import os\n",
    "from IPython.display import clear_output\n",
    "import numpy as np\n",
    "import joblib\n",
    "import contextlib\n",
    "import multiprocessing\n",
    "import pyarrow as pa\n",
    "import pyarrow.parquet as pq\n",
    "import json\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv(override=True)\n",
    "\n",
    "# Import jupysql Jupyter extension to create SQL cells\n",
    "%load_ext sql\n",
    "%reload_ext sql\n",
    "\n",
    "# Set configrations on jupysql to directly output data to Pandas and to simplify the output that is printed to the notebook.\n",
    "%config SqlMagic.autopandas = True\n",
    "%config SqlMagic.feedback = False\n",
    "%config SqlMagic.displaycon = False\n",
    "%config SqlMagic.autolimit = False\n",
    "\n",
    "# Connect jupysql to DuckDB using a SQLAlchemy-style connection string. Either connect to an in memory DuckDB, or a file backed db.\n",
    "%sql duckdb:///:memory:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "BUCKET = 'payless.health'\n",
    "REGION = 'us-east-1'\n",
    "PREFIX = \"hospital_price_transparency\"\n",
    "\n",
    "access_key = os.getenv('AWS_ACCESS_KEY_ID')\n",
    "secret_key = os.getenv('AWS_SECRET_ACCESS_KEY')\n",
    "s3 = boto3.client('s3', aws_access_key_id=access_key, aws_secret_access_key=secret_key)\n",
    "s3_resource = boto3.resource('s3', aws_access_key_id=access_key, aws_secret_access_key=secret_key)\n",
    "bucket = s3_resource.Bucket(BUCKET)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# simple tqdm wrapper for joblib\n",
    "@contextlib.contextmanager\n",
    "def tqdm_joblib(tqdm_object):\n",
    "    \"\"\"Context manager to patch joblib to report into tqdm progress bar given as argument\"\"\"\n",
    "\n",
    "    class TqdmBatchCompletionCallback(joblib.parallel.BatchCompletionCallBack):\n",
    "        def __call__(self, *args, **kwargs):\n",
    "            tqdm_object.update(n=self.batch_size)\n",
    "            return super().__call__(*args, **kwargs)\n",
    "\n",
    "    old_batch_callback = joblib.parallel.BatchCompletionCallBack\n",
    "    joblib.parallel.BatchCompletionCallBack = TqdmBatchCompletionCallback\n",
    "    try:\n",
    "        yield tqdm_object\n",
    "    finally:\n",
    "        joblib.parallel.BatchCompletionCallBack = old_batch_callback\n",
    "        tqdm_object.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if data folder doesn't exist, create it\n",
    "if not os.path.exists('../data'):\n",
    "    os.makedirs('../data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 369 raw files to download from S3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Raw csvs: 100%|██████████| 369/369 [4:44:12<00:00, 46.21s/it]   \n"
     ]
    }
   ],
   "source": [
    "# process the csv files, fix the encoding, and upload to S3 in processed folder\n",
    "urls = []\n",
    "# iterate over the list of objects\n",
    "for obj in s3.list_objects(Bucket=BUCKET)['Contents']:\n",
    "    # filter the objects with the prefix and suffix needed\n",
    "    if obj['Key'].startswith(PREFIX) and obj['Key'].endswith('.csv') and \"processed\" not in obj['Key']:\n",
    "        url = f\"https://s3.amazonaws.com/{BUCKET}/\" + obj['Key']\n",
    "        urls.append(url)\n",
    "\n",
    "print(f\"Found {len(urls)} raw files to download from S3\")\n",
    "\n",
    "def process_file(url):\n",
    "    file = url.split(\"/\")[-1]\n",
    "    key = f\"{PREFIX}/processed/{file}\"\n",
    "\n",
    "    # if already processed, skip\n",
    "    if len(list(s3_resource.Bucket(BUCKET).objects.filter(Prefix=key))) == 0:\n",
    "        try:\n",
    "            r = requests.get(url, allow_redirects=True)\n",
    "            encoding = chardet.detect(r.content)['encoding']\n",
    "            open(f\"../data/{file}\", 'wb').write(r.content)\n",
    "            df = pd.read_csv(f\"../data/{file}\", encoding=encoding, engine='python')\n",
    "            df.to_csv(f\"../data/{file}\", index=False, encoding='utf-8')\n",
    "            s3_resource.Bucket(BUCKET).upload_file(f\"../data/{file}\", key)\n",
    "            os.remove(f\"../data/{file}\") # delete the file from local\n",
    "        except:\n",
    "            return\n",
    "    \n",
    "with tqdm_joblib(tqdm(desc=\"Processing Raw csvs\", total=len(urls))):\n",
    "    joblib.Parallel(n_jobs=multiprocessing.cpu_count() - 1, backend=\"threading\")(joblib.delayed(process_file)(url) for url in urls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 333 processed files to download from S3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 333/333 [08:31<00:00,  1.54s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 323 files with headers\n",
      "Found 10 files without errors\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "processed_urls = []\n",
    "\n",
    "PREFIX = \"hospital_price_transparency/processed\"\n",
    "# iterate over the list of objects\n",
    "for obj in s3_resource.Bucket(BUCKET).objects.all():\n",
    "    # filter the objects with the prefix and suffix needed\n",
    "    if PREFIX in obj.key and obj.key.endswith('.csv'):\n",
    "        url = f\"https://s3.amazonaws.com/{BUCKET}/\" + obj.key\n",
    "        processed_urls.append(url)\n",
    "\n",
    "print(f\"Found {len(processed_urls)} processed files to download from S3\")\n",
    "\n",
    "headers = []\n",
    "bad_urls = []\n",
    "\n",
    "for url in tqdm(processed_urls):\n",
    "    try:\n",
    "        df = pd.read_csv(url, header=None, nrows=10)\n",
    "        CCN = url.split(\"/\")[-1][:6]\n",
    "        # simple heuristic to find header row\n",
    "        string_rows = [\",\".join([str(x) for x in df.iloc[i, :]]) for i in range(10)]\n",
    "        header_row = np.argmax([len(x) for x in string_rows])\n",
    "        headers.append([CCN, df.iloc[header_row, :].tolist()])\n",
    "    except:\n",
    "        bad_urls.append(url)\n",
    "        continue\n",
    "\n",
    "print(f\"Found {len(headers)} files with headers\")\n",
    "print(f\"Found {len(bad_urls)} files without errors\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate a parquet file with the CCN and the column names (headers)\n",
    "CCN_column = []\n",
    "header_column = []\n",
    "for header in headers:\n",
    "    CCN = header[0]\n",
    "    for column in header[1]:\n",
    "        CCN_column.append(CCN)\n",
    "        header_column.append(str(column))\n",
    "\n",
    "CCN_column = pa.array(CCN_column)\n",
    "header_column = pa.array(header_column)\n",
    "\n",
    "table = pa.Table.from_arrays([CCN_column, header_column], names=['CCN', 'source_column_name'])\n",
    "pq.write_table(table, '../hospital_price_transparency_ccn_column_names.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 11295/11295 [00:00<00:00, 288091.70it/s]\n"
     ]
    }
   ],
   "source": [
    "# convert to label studio json format for labeling\n",
    "label_studio_json = []\n",
    "for i in tqdm(range(len(header_column))):\n",
    "    label_studio_json.append({\n",
    "        \"data\": {\n",
    "            \"CCN\": str(CCN_column[i]),\n",
    "            \"text\": str(header_column[i])\n",
    "        },\n",
    "        \"id\": i\n",
    "    })\n",
    "\n",
    "with open('../hospital_price_transparency_ccn_column_names.json', 'w') as f:\n",
    "    json.dump(label_studio_json, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for url in urls:\n",
    "    file = url.split(\"%2F\")[1]\n",
    "    full_path = os.path.abspath(f\"../data/{file}\")\n",
    "    query = f\"SELECT * FROM read_csv_auto('{full_path}', header=False) LIMIT 10;\"\n",
    "    res = %sql {{query}}\n",
    "    for i in range(10):\n",
    "        vals = list(res.iloc[i, :10])\n",
    "        print(i, \", \".join([str(val) for val in vals]))\n",
    "    print(\"Enter row index to use as column names:\")\n",
    "    row_indexes.append(input())\n",
    "    clear_output(wait=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "duck",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "214ba8973c16ed8191ff2be19c197ad5f0fdb15fa7d941bed5efca6039da4a5d"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
